{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e158b210",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "root = './data'\n",
    "if not os.path.exists(root):\n",
    "    os.mkdir(root)\n",
    "    \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0ff213",
   "metadata": {},
   "source": [
    "### Gradient Reversal Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17e14463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient reversal\n",
    "from torch.autograd import Function\n",
    "\n",
    "# Autograd Function objects are what record operation history on tensors\n",
    "# and define formulas for the forwawrd and backprop.\n",
    "\n",
    "class GradientReversalFn(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):\n",
    "        # Store context for backprop\n",
    "        ctx.alpha = alpha\n",
    "        \n",
    "        # Forward pass is a no-op\n",
    "        return x.view_as(x)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # Backward pass is just to -alpha the gradient\n",
    "        output = grad_output.neg() * ctx.alpha\n",
    "        \n",
    "        # Must return same number as inputs to forward()\n",
    "        return output, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613ff8ee",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "77089fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 88, 88])\n"
     ]
    }
   ],
   "source": [
    "class SeparableConv2d(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, bias=False):\n",
    "        super(SeparableConv2d, self).__init__()\n",
    "        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size=kernel_size, \n",
    "                                   groups=in_channels, bias=bias, padding=1)\n",
    "        self.pointwise = nn.Conv2d(in_channels, out_channels, \n",
    "                                   kernel_size=1, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.depthwise(x)\n",
    "        out = self.pointwise(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class DASemantic(nn.Module):\n",
    "    def __init__(self, img_size, num_classes):\n",
    "        super(DASemantic, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.entry = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, stride=2),\n",
    "            nn.BatchNorm2d(num_features=32, eps=1e-3, momentum=0.99),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            SeparableConv2d(32, 32, 3),\n",
    "            nn.BatchNorm2d(num_features=32, eps=1e-3, momentum=0.99),\n",
    "            nn.ReLU(),\n",
    "            SeparableConv2d(32, 32, 3),\n",
    "            nn.BatchNorm2d(num_features=32, eps=1e-3, momentum=0.99),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3, stride=2, padding=1)\n",
    "        )\n",
    "        self.residual1 = nn.Conv2d(32, 32, 1, stride=2)\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            SeparableConv2d(32, 64, 3),\n",
    "            nn.BatchNorm2d(num_features=64, eps=1e-3, momentum=0.99),\n",
    "            nn.ReLU(),\n",
    "            SeparableConv2d(64, 64, 3),\n",
    "            nn.BatchNorm2d(num_features=64, eps=1e-3, momentum=0.99),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3, stride=2, padding=1)\n",
    "        )\n",
    "        self.residual2 = nn.Conv2d(32, 64, 1, stride=2)\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            SeparableConv2d(64, 128, 3),\n",
    "            nn.BatchNorm2d(num_features=128, eps=1e-3, momentum=0.99),\n",
    "            nn.ReLU(),\n",
    "            SeparableConv2d(128, 128, 3),\n",
    "            nn.BatchNorm2d(num_features=128, eps=1e-3, momentum=0.99),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3, stride=2, padding=1)\n",
    "        )\n",
    "        self.residual3 = nn.Conv2d(64, 128, 1, stride=2)\n",
    "        self.domain_classifier = nn.Sequential(\n",
    "            nn.Linear(128*45*45, 100), nn.BatchNorm1d(100),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(100,2),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "        self.upsample1 = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(num_features=128, eps=1e-3, momentum=0.99),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(num_features=128, eps=1e-3, momentum=0.99),\n",
    "            nn.UpsamplingNearest2d(scale_factor=2)\n",
    "        )\n",
    "        self.residual4 = nn.Sequential(\n",
    "            nn.UpsamplingNearest2d(scale_factor=2),\n",
    "            nn.Conv2d(128,128,1)\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            SeparableConv2d(128, 64, 3),\n",
    "            nn.BatchNorm2d(num_features=64, eps=1e-3, momentum=0.99),\n",
    "            nn.ReLU(),\n",
    "            SeparableConv2d(64, 32, 3),\n",
    "            nn.BatchNorm2d(num_features=32, eps=1e-3, momentum=0.99),\n",
    "            nn.Conv2d(32, num_classes, 3)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, grl_lambda):\n",
    "        # Handle single-channel input by expanding singleton dim\n",
    "        x = x.expand(x.data.shape[0], 3, self.img_size, self.img_size)\n",
    "        \n",
    "        # Entry block\n",
    "        x = self.entry(x)\n",
    "        previous_block_activation = x\n",
    "        \n",
    "        # Feature Depth\n",
    "        # block 1\n",
    "        x = self.block1(x)\n",
    "        #print('Post Block1: ' + str(x.shape))\n",
    "        previous_block_activation = self.residual1(previous_block_activation)\n",
    "        #print('Post Residual1: ' + str(previous_block_activation.shape))\n",
    "        x = torch.add(x, previous_block_activation)\n",
    "        #print('Post add1: ' + str(x.shape))\n",
    "        previous_block_activation = x\n",
    "        # block 2\n",
    "        x = self.block2(x)\n",
    "        #print('Post Block2: ' + str(x.shape))\n",
    "        previous_block_activation = self.residual2(previous_block_activation)\n",
    "        #print('Post Residual2: ' + str(previous_block_activation.shape))\n",
    "        x = torch.add(x, previous_block_activation)\n",
    "        #print('Post add2: ' + str(x.shape))\n",
    "        previous_block_activation = x\n",
    "        # block 3\n",
    "        x = self.block3(x)\n",
    "        #print('Post Block3: ' + str(x.shape))\n",
    "        previous_block_activation = self.residual3(previous_block_activation)\n",
    "        #print('Post Residual3: ' + str(previous_block_activation.shape))\n",
    "        x = torch.add(x, previous_block_activation)\n",
    "        #print('Post add3: ' + str(x.shape))\n",
    "        previous_block_activation = x\n",
    "        \n",
    "        # Gradient Reversal\n",
    "        features = x.view(-1, 128 * 45 * 45)\n",
    "        #print('GRL Feature vector: ' + str(features.shape))\n",
    "        reverse_features = GradientReversalFn.apply(features, grl_lambda)\n",
    "        #print('GRL Reverse Feature vector: ' + str(reverse_features.shape))\n",
    "        \n",
    "        # Upsampling\n",
    "        # upsample 1\n",
    "        x = self.upsample1(x)\n",
    "        #print('Post UpSample1: ' + str(x.shape))\n",
    "        previous_block_activation = self.residual4(previous_block_activation)\n",
    "        #print('Post Residual4: ' + str(previous_block_activation.shape))\n",
    "        x = torch.add(x, previous_block_activation)\n",
    "        #print('Post add4: ' + str(x.shape))\n",
    "        \n",
    "        # Head\n",
    "        output = self.head(x)\n",
    "        #print('output: ' + str(output.shape))\n",
    "        # Domain Pred\n",
    "        domain_pred = self.domain_classifier(reverse_features)\n",
    "        #print('domain_pred: ' + str(domain_pred.shape))\n",
    "        \n",
    "        return output, domain_pred\n",
    "\n",
    "model = DASemantic(720, 2).cuda()\n",
    "batch = torch.rand(2,3,720,720).cuda()\n",
    "print(model(batch, 1.)[0].shape)\n",
    "\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4367ca6c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 32, 359, 359])\n",
      "Post Block1: torch.Size([2, 32, 180, 180])\n",
      "Post Residual1: torch.Size([2, 32, 180, 180])\n",
      "Post add1: torch.Size([2, 32, 180, 180])\n",
      "Post Block2: torch.Size([2, 64, 90, 90])\n",
      "Post Residual2: torch.Size([2, 64, 90, 90])\n",
      "Post add2: torch.Size([2, 64, 90, 90])\n",
      "Post Block3: torch.Size([2, 128, 45, 45])\n",
      "Post Residual3: torch.Size([2, 128, 45, 45])\n",
      "Post add3: torch.Size([2, 128, 45, 45])\n",
      "GRL Feature vector: torch.Size([2, 259200])\n",
      "GRL Reverse Feature vector: torch.Size([2, 259200])\n",
      "Post UpSample1: torch.Size([2, 128, 90, 90])\n",
      "Post Residual4: torch.Size([2, 128, 90, 90])\n",
      "Post add4: torch.Size([2, 128, 90, 90])\n",
      "output: torch.Size([2, 2, 88, 88])\n",
      "domain_pred: torch.Size([2, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: The shape inference of prim::PythonOp type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::PythonOp type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "ONNX export failed: Couldn't export Python operator GradientReversalFn\n\nDefined at:\n/tmp/ipykernel_914631/3131590279.py(125): forward\n/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py(1098): _slow_forward\n/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py(1110): _call_impl\n/home/ubuntu/.local/lib/python3.8/site-packages/torch/jit/_trace.py(118): wrapper\n/home/ubuntu/.local/lib/python3.8/site-packages/torch/jit/_trace.py(127): forward\n/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py(1110): _call_impl\n/home/ubuntu/.local/lib/python3.8/site-packages/torch/jit/_trace.py(1166): _get_trace_graph\n/home/ubuntu/.local/lib/python3.8/site-packages/torch/onnx/utils.py(391): _trace_and_get_graph_from_model\n/home/ubuntu/.local/lib/python3.8/site-packages/torch/onnx/utils.py(440): _create_jit_graph\n/home/ubuntu/.local/lib/python3.8/site-packages/torch/onnx/utils.py(499): _model_to_graph\n/home/ubuntu/.local/lib/python3.8/site-packages/torch/onnx/utils.py(719): _export\n/home/ubuntu/.local/lib/python3.8/site-packages/torch/onnx/utils.py(118): export\n/home/ubuntu/.local/lib/python3.8/site-packages/torch/onnx/__init__.py(305): export\n/tmp/ipykernel_914631/343856551.py(5): <module>\n/home/ubuntu/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py(3251): run_code\n/home/ubuntu/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py(3191): run_ast_nodes\n/home/ubuntu/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py(3012): run_cell_async\n/home/ubuntu/.local/lib/python3.8/site-packages/IPython/core/async_helpers.py(129): _pseudo_sync_runner\n/home/ubuntu/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py(2814): _run_cell\n/home/ubuntu/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py(2768): run_cell\n/home/ubuntu/.local/lib/python3.8/site-packages/ipykernel/zmqshell.py(532): run_cell\n/home/ubuntu/.local/lib/python3.8/site-packages/ipykernel/ipkernel.py(353): do_execute\n/home/ubuntu/.local/lib/python3.8/site-packages/ipykernel/kernelbase.py(652): execute_request\n/home/ubuntu/.local/lib/python3.8/site-packages/ipykernel/kernelbase.py(357): dispatch_shell\n/home/ubuntu/.local/lib/python3.8/site-packages/ipykernel/kernelbase.py(450): process_one\n/home/ubuntu/.local/lib/python3.8/site-packages/ipykernel/kernelbase.py(461): dispatch_queue\n/usr/lib/python3.8/asyncio/events.py(81): _run\n/usr/lib/python3.8/asyncio/base_events.py(1859): _run_once\n/usr/lib/python3.8/asyncio/base_events.py(570): run_forever\n/home/ubuntu/.local/lib/python3.8/site-packages/tornado/platform/asyncio.py(199): start\n/home/ubuntu/.local/lib/python3.8/site-packages/ipykernel/kernelapp.py(677): start\n/home/ubuntu/.local/lib/python3.8/site-packages/traitlets/config/application.py(846): launch_instance\n/home/ubuntu/.local/lib/python3.8/site-packages/ipykernel_launcher.py(16): <module>\n/usr/lib/python3.8/runpy.py(87): _run_code\n/usr/lib/python3.8/runpy.py(194): _run_module_as_main\n\n\nGraph we tried to export:\ngraph(%input : Float(2, 3, 720, 720, strides=[1555200, 518400, 720, 1], requires_grad=0, device=cuda:0),\n      %prim::PythonOp_1 : Double(requires_grad=0, device=cpu),\n      %block1.1.depthwise.weight : Float(32, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=1, device=cuda:0),\n      %block1.4.depthwise.weight : Float(32, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=1, device=cuda:0),\n      %residual1.weight : Float(32, 32, 1, 1, strides=[32, 1, 1, 1], requires_grad=1, device=cuda:0),\n      %residual1.bias : Float(32, strides=[1], requires_grad=1, device=cuda:0),\n      %block2.1.depthwise.weight : Float(32, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=1, device=cuda:0),\n      %block2.4.depthwise.weight : Float(64, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=1, device=cuda:0),\n      %residual2.weight : Float(64, 32, 1, 1, strides=[32, 1, 1, 1], requires_grad=1, device=cuda:0),\n      %residual2.bias : Float(64, strides=[1], requires_grad=1, device=cuda:0),\n      %block3.1.depthwise.weight : Float(64, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=1, device=cuda:0),\n      %block3.4.depthwise.weight : Float(128, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=1, device=cuda:0),\n      %residual3.weight : Float(128, 64, 1, 1, strides=[64, 1, 1, 1], requires_grad=1, device=cuda:0),\n      %residual3.bias : Float(128, strides=[1], requires_grad=1, device=cuda:0),\n      %domain_classifier.0.weight : Float(100, 259200, strides=[259200, 1], requires_grad=1, device=cuda:0),\n      %domain_classifier.0.bias : Float(100, strides=[1], requires_grad=1, device=cuda:0),\n      %domain_classifier.1.weight : Float(100, strides=[1], requires_grad=1, device=cuda:0),\n      %domain_classifier.1.bias : Float(100, strides=[1], requires_grad=1, device=cuda:0),\n      %domain_classifier.1.running_mean : Float(100, strides=[1], requires_grad=0, device=cuda:0),\n      %domain_classifier.1.running_var : Float(100, strides=[1], requires_grad=0, device=cuda:0),\n      %domain_classifier.3.weight : Float(2, 100, strides=[100, 1], requires_grad=1, device=cuda:0),\n      %domain_classifier.3.bias : Float(2, strides=[1], requires_grad=1, device=cuda:0),\n      %upsample1.1.weight : Float(128, 128, 3, 3, strides=[1152, 9, 3, 1], requires_grad=1, device=cuda:0),\n      %upsample1.1.bias : Float(128, strides=[1], requires_grad=1, device=cuda:0),\n      %upsample1.2.weight : Float(128, strides=[1], requires_grad=1, device=cuda:0),\n      %upsample1.2.bias : Float(128, strides=[1], requires_grad=1, device=cuda:0),\n      %upsample1.2.running_mean : Float(128, strides=[1], requires_grad=0, device=cuda:0),\n      %upsample1.2.running_var : Float(128, strides=[1], requires_grad=0, device=cuda:0),\n      %upsample1.4.weight : Float(128, 128, 3, 3, strides=[1152, 9, 3, 1], requires_grad=1, device=cuda:0),\n      %upsample1.4.bias : Float(128, strides=[1], requires_grad=1, device=cuda:0),\n      %upsample1.5.weight : Float(128, strides=[1], requires_grad=1, device=cuda:0),\n      %upsample1.5.bias : Float(128, strides=[1], requires_grad=1, device=cuda:0),\n      %upsample1.5.running_mean : Float(128, strides=[1], requires_grad=0, device=cuda:0),\n      %upsample1.5.running_var : Float(128, strides=[1], requires_grad=0, device=cuda:0),\n      %residual4.1.weight : Float(128, 128, 1, 1, strides=[128, 1, 1, 1], requires_grad=1, device=cuda:0),\n      %residual4.1.bias : Float(128, strides=[1], requires_grad=1, device=cuda:0),\n      %head.1.depthwise.weight : Float(128, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=1, device=cuda:0),\n      %head.4.depthwise.weight : Float(64, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=1, device=cuda:0),\n      %head.6.weight : Float(2, 32, 3, 3, strides=[288, 9, 3, 1], requires_grad=1, device=cuda:0),\n      %head.6.bias : Float(2, strides=[1], requires_grad=1, device=cuda:0),\n      %onnx::Conv_182 : Float(32, 3, 3, 3, strides=[27, 9, 3, 1], requires_grad=0, device=cuda:0),\n      %onnx::Conv_183 : Float(32, strides=[1], requires_grad=0, device=cuda:0),\n      %onnx::Conv_185 : Float(32, 32, 1, 1, strides=[32, 1, 1, 1], requires_grad=0, device=cuda:0),\n      %onnx::Conv_186 : Float(32, strides=[1], requires_grad=0, device=cuda:0),\n      %onnx::Conv_188 : Float(32, 32, 1, 1, strides=[32, 1, 1, 1], requires_grad=0, device=cuda:0),\n      %onnx::Conv_189 : Float(32, strides=[1], requires_grad=0, device=cuda:0),\n      %onnx::Conv_191 : Float(64, 32, 1, 1, strides=[32, 1, 1, 1], requires_grad=0, device=cuda:0),\n      %onnx::Conv_192 : Float(64, strides=[1], requires_grad=0, device=cuda:0),\n      %onnx::Conv_194 : Float(64, 64, 1, 1, strides=[64, 1, 1, 1], requires_grad=0, device=cuda:0),\n      %onnx::Conv_195 : Float(64, strides=[1], requires_grad=0, device=cuda:0),\n      %onnx::Conv_197 : Float(128, 64, 1, 1, strides=[64, 1, 1, 1], requires_grad=0, device=cuda:0),\n      %onnx::Conv_198 : Float(128, strides=[1], requires_grad=0, device=cuda:0),\n      %onnx::Conv_200 : Float(128, 128, 1, 1, strides=[128, 1, 1, 1], requires_grad=0, device=cuda:0),\n      %onnx::Conv_201 : Float(128, strides=[1], requires_grad=0, device=cuda:0),\n      %onnx::Conv_203 : Float(64, 128, 1, 1, strides=[128, 1, 1, 1], requires_grad=0, device=cuda:0),\n      %onnx::Conv_204 : Float(64, strides=[1], requires_grad=0, device=cuda:0),\n      %onnx::Conv_206 : Float(32, 64, 1, 1, strides=[64, 1, 1, 1], requires_grad=0, device=cuda:0),\n      %onnx::Conv_207 : Float(32, strides=[1], requires_grad=0, device=cuda:0),\n      %onnx::ConstantOfShape_208 : Long(1, strides=[1], requires_grad=0, device=cpu),\n      %onnx::Resize_209 : Float(4, strides=[1], requires_grad=0, device=cpu),\n      %onnx::Resize_210 : Float(4, strides=[1], requires_grad=0, device=cpu)):\n  %onnx::Where_98 : Long(4, strides=[1], device=cpu) = onnx::Constant[value=   2    3  720  720 [ CPULongType{4} ]]() # /tmp/ipykernel_914631/3131590279.py:89:0\n  %onnx::Mul_100 : Long(4, device=cpu) = onnx::ConstantOfShape[value={1}](%onnx::ConstantOfShape_208) # /tmp/ipykernel_914631/3131590279.py:89:0\n  %onnx::Mul_101 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}]() # /tmp/ipykernel_914631/3131590279.py:89:0\n  %onnx::Equal_102 : Long(4, strides=[1], device=cpu) = onnx::Mul(%onnx::Mul_100, %onnx::Mul_101) # /tmp/ipykernel_914631/3131590279.py:89:0\n  %onnx::Equal_103 : Long(4, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value=   2    3  720  720 [ CPULongType{4} ]]() # /tmp/ipykernel_914631/3131590279.py:89:0\n  %onnx::Where_104 : Bool(4, strides=[1], device=cpu) = onnx::Equal(%onnx::Equal_103, %onnx::Equal_102) # /tmp/ipykernel_914631/3131590279.py:89:0\n  %onnx::Expand_105 : Long(4, strides=[1], device=cpu) = onnx::Where(%onnx::Where_104, %onnx::Mul_100, %onnx::Where_98) # /tmp/ipykernel_914631/3131590279.py:89:0\n  %input.1 : Float(2, 3, 720, 720, strides=[1555200, 518400, 720, 1], requires_grad=0, device=cuda:0) = onnx::Expand(%input, %onnx::Expand_105) # /tmp/ipykernel_914631/3131590279.py:89:0\n  %input.7 : Float(2, 32, 359, 359, strides=[4124192, 128881, 359, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2]](%input.1, %onnx::Conv_182, %onnx::Conv_183) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:443:0\n  %input.11 : Float(2, 32, 359, 359, strides=[4124192, 128881, 359, 1], requires_grad=1, device=cuda:0) = onnx::Relu(%input.7) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/functional.py:1442:0\n  %input.15 : Float(2, 32, 359, 359, strides=[4124192, 128881, 359, 1], requires_grad=1, device=cuda:0) = onnx::Relu(%input.11) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/functional.py:1442:0\n  %input.19 : Float(2, 32, 359, 359, strides=[4124192, 128881, 359, 1], requires_grad=0, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=32, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%input.15, %block1.1.depthwise.weight) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:443:0\n  %input.27 : Float(2, 32, 359, 359, strides=[4124192, 128881, 359, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%input.19, %onnx::Conv_185, %onnx::Conv_186) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:443:0\n  %input.31 : Float(2, 32, 359, 359, strides=[4124192, 128881, 359, 1], requires_grad=1, device=cuda:0) = onnx::Relu(%input.27) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/functional.py:1442:0\n  %input.35 : Float(2, 32, 359, 359, strides=[4124192, 128881, 359, 1], requires_grad=0, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=32, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%input.31, %block1.4.depthwise.weight) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:443:0\n  %input.43 : Float(2, 32, 359, 359, strides=[4124192, 128881, 359, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%input.35, %onnx::Conv_188, %onnx::Conv_189) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:443:0\n  %onnx::MaxPool_118 : Float(2, 32, 359, 359, strides=[4124192, 128881, 359, 1], requires_grad=1, device=cuda:0) = onnx::Relu(%input.43) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/functional.py:1442:0\n  %onnx::Add_119 : Float(2, 32, 180, 180, strides=[1036800, 32400, 180, 1], requires_grad=1, device=cuda:0) = onnx::MaxPool[ceil_mode=0, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%onnx::MaxPool_118) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/functional.py:797:0\n  %onnx::Add_120 : Float(2, 32, 180, 180, strides=[1036800, 32400, 180, 1], requires_grad=0, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[2, 2]](%input.11, %residual1.weight, %residual1.bias) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:443:0\n  %input.47 : Float(2, 32, 180, 180, strides=[1036800, 32400, 180, 1], requires_grad=1, device=cuda:0) = onnx::Add(%onnx::Add_119, %onnx::Add_120) # /tmp/ipykernel_914631/3131590279.py:102:0\n  %input.51 : Float(2, 32, 180, 180, strides=[1036800, 32400, 180, 1], requires_grad=1, device=cuda:0) = onnx::Relu(%input.47) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/functional.py:1442:0\n  %input.55 : Float(2, 32, 180, 180, strides=[1036800, 32400, 180, 1], requires_grad=0, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=32, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%input.51, %block2.1.depthwise.weight) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:443:0\n  %input.63 : Float(2, 64, 180, 180, strides=[2073600, 32400, 180, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%input.55, %onnx::Conv_191, %onnx::Conv_192) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:443:0\n  %input.67 : Float(2, 64, 180, 180, strides=[2073600, 32400, 180, 1], requires_grad=1, device=cuda:0) = onnx::Relu(%input.63) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/functional.py:1442:0\n  %input.71 : Float(2, 64, 180, 180, strides=[2073600, 32400, 180, 1], requires_grad=0, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=64, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%input.67, %block2.4.depthwise.weight) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:443:0\n  %input.79 : Float(2, 64, 180, 180, strides=[2073600, 32400, 180, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%input.71, %onnx::Conv_194, %onnx::Conv_195) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:443:0\n  %onnx::MaxPool_130 : Float(2, 64, 180, 180, strides=[2073600, 32400, 180, 1], requires_grad=1, device=cuda:0) = onnx::Relu(%input.79) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/functional.py:1442:0\n  %onnx::Add_131 : Float(2, 64, 90, 90, strides=[518400, 8100, 90, 1], requires_grad=1, device=cuda:0) = onnx::MaxPool[ceil_mode=0, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%onnx::MaxPool_130) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/functional.py:797:0\n  %onnx::Add_132 : Float(2, 64, 90, 90, strides=[518400, 8100, 90, 1], requires_grad=0, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[2, 2]](%input.47, %residual2.weight, %residual2.bias) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:443:0\n  %input.83 : Float(2, 64, 90, 90, strides=[518400, 8100, 90, 1], requires_grad=1, device=cuda:0) = onnx::Add(%onnx::Add_131, %onnx::Add_132) # /tmp/ipykernel_914631/3131590279.py:110:0\n  %input.87 : Float(2, 64, 90, 90, strides=[518400, 8100, 90, 1], requires_grad=1, device=cuda:0) = onnx::Relu(%input.83) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/functional.py:1442:0\n  %input.91 : Float(2, 64, 90, 90, strides=[518400, 8100, 90, 1], requires_grad=0, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=64, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%input.87, %block3.1.depthwise.weight) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:443:0\n  %input.99 : Float(2, 128, 90, 90, strides=[1036800, 8100, 90, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%input.91, %onnx::Conv_197, %onnx::Conv_198) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:443:0\n  %input.103 : Float(2, 128, 90, 90, strides=[1036800, 8100, 90, 1], requires_grad=1, device=cuda:0) = onnx::Relu(%input.99) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/functional.py:1442:0\n  %input.107 : Float(2, 128, 90, 90, strides=[1036800, 8100, 90, 1], requires_grad=0, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=128, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%input.103, %block3.4.depthwise.weight) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:443:0\n  %input.115 : Float(2, 128, 90, 90, strides=[1036800, 8100, 90, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%input.107, %onnx::Conv_200, %onnx::Conv_201) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:443:0\n  %onnx::MaxPool_142 : Float(2, 128, 90, 90, strides=[1036800, 8100, 90, 1], requires_grad=1, device=cuda:0) = onnx::Relu(%input.115) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/functional.py:1442:0\n  %onnx::Add_143 : Float(2, 128, 45, 45, strides=[259200, 2025, 45, 1], requires_grad=1, device=cuda:0) = onnx::MaxPool[ceil_mode=0, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%onnx::MaxPool_142) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/functional.py:797:0\n  %onnx::Add_144 : Float(2, 128, 45, 45, strides=[259200, 2025, 45, 1], requires_grad=0, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[2, 2]](%input.83, %residual3.weight, %residual3.bias) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:443:0\n  %input.119 : Float(2, 128, 45, 45, strides=[259200, 2025, 45, 1], requires_grad=1, device=cuda:0) = onnx::Add(%onnx::Add_143, %onnx::Add_144) # /tmp/ipykernel_914631/3131590279.py:118:0\n  %onnx::Reshape_146 : Long(2, strides=[1], device=cpu) = onnx::Constant[value=     -1  259200 [ CPULongType{2} ]]() # /tmp/ipykernel_914631/3131590279.py:123:0\n  %features : Float(2, 259200, strides=[259200, 1], requires_grad=1, device=cuda:0) = onnx::Reshape(%input.119, %onnx::Reshape_146) # /tmp/ipykernel_914631/3131590279.py:123:0\n  %onnx::Gemm_148 : Float(*, *, strides=[259200, 1], requires_grad=1, device=cuda:0) = ^GradientReversalFn()(%features, %prim::PythonOp_1) # /tmp/ipykernel_914631/3131590279.py:125:0\n  %onnx::ConvTranspose_149 : Float(2, 128, 45, 45, strides=[259200, 2025, 45, 1], requires_grad=1, device=cuda:0) = onnx::Relu(%input.119) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/functional.py:1442:0\n  %input.123 : Float(2, 128, 45, 45, strides=[259200, 2025, 45, 1], requires_grad=0, device=cuda:0) = onnx::ConvTranspose[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%onnx::ConvTranspose_149, %upsample1.1.weight, %upsample1.1.bias) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:925:0\n  %input.127 : Float(2, 128, 45, 45, strides=[259200, 2025, 45, 1], requires_grad=1, device=cuda:0) = onnx::BatchNormalization[epsilon=0.001, momentum=0.010000000000000009](%input.123, %upsample1.2.weight, %upsample1.2.bias, %upsample1.2.running_mean, %upsample1.2.running_var) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/functional.py:2421:0\n  %onnx::ConvTranspose_152 : Float(2, 128, 45, 45, strides=[259200, 2025, 45, 1], requires_grad=1, device=cuda:0) = onnx::Relu(%input.127) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/functional.py:1442:0\n  %input.131 : Float(2, 128, 45, 45, strides=[259200, 2025, 45, 1], requires_grad=0, device=cuda:0) = onnx::ConvTranspose[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%onnx::ConvTranspose_152, %upsample1.4.weight, %upsample1.4.bias) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:925:0\n  %input.135 : Float(2, 128, 45, 45, strides=[259200, 2025, 45, 1], requires_grad=1, device=cuda:0) = onnx::BatchNormalization[epsilon=0.001, momentum=0.010000000000000009](%input.131, %upsample1.5.weight, %upsample1.5.bias, %upsample1.5.running_mean, %upsample1.5.running_var) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/functional.py:2421:0\n  %onnx::Resize_158 : Float(0, strides=[1], device=cpu) = onnx::Constant[value=[ CPUFloatType{0} ]]() # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/functional.py:3891:0\n  %onnx::Add_159 : Float(2, 128, 90, 90, strides=[1036800, 8100, 90, 1], requires_grad=1, device=cuda:0) = onnx::Resize[coordinate_transformation_mode=\"asymmetric\", cubic_coeff_a=-0.75, mode=\"nearest\", nearest_mode=\"floor\"](%input.135, %onnx::Resize_158, %onnx::Resize_209) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/functional.py:3891:0\n  %onnx::Resize_163 : Float(0, strides=[1], device=cpu) = onnx::Constant[value=[ CPUFloatType{0} ]]() # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/functional.py:3891:0\n  %input.139 : Float(2, 128, 90, 90, strides=[1036800, 8100, 90, 1], requires_grad=1, device=cuda:0) = onnx::Resize[coordinate_transformation_mode=\"asymmetric\", cubic_coeff_a=-0.75, mode=\"nearest\", nearest_mode=\"floor\"](%input.119, %onnx::Resize_163, %onnx::Resize_210) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/functional.py:3891:0\n  %onnx::Add_165 : Float(2, 128, 90, 90, strides=[1036800, 8100, 90, 1], requires_grad=0, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%input.139, %residual4.1.weight, %residual4.1.bias) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:443:0\n  %input.143 : Float(2, 128, 90, 90, strides=[1036800, 8100, 90, 1], requires_grad=1, device=cuda:0) = onnx::Add(%onnx::Add_159, %onnx::Add_165) # /tmp/ipykernel_914631/3131590279.py:134:0\n  %input.147 : Float(2, 128, 90, 90, strides=[1036800, 8100, 90, 1], requires_grad=1, device=cuda:0) = onnx::Relu(%input.143) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/functional.py:1442:0\n  %input.151 : Float(2, 128, 90, 90, strides=[1036800, 8100, 90, 1], requires_grad=0, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=128, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%input.147, %head.1.depthwise.weight) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:443:0\n  %input.159 : Float(2, 64, 90, 90, strides=[518400, 8100, 90, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%input.151, %onnx::Conv_203, %onnx::Conv_204) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:443:0\n  %input.163 : Float(2, 64, 90, 90, strides=[518400, 8100, 90, 1], requires_grad=1, device=cuda:0) = onnx::Relu(%input.159) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/functional.py:1442:0\n  %input.167 : Float(2, 64, 90, 90, strides=[518400, 8100, 90, 1], requires_grad=0, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=64, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%input.163, %head.4.depthwise.weight) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:443:0\n  %input.175 : Float(2, 32, 90, 90, strides=[259200, 8100, 90, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%input.167, %onnx::Conv_206, %onnx::Conv_207) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:443:0\n  %output : Float(2, 2, 88, 88, strides=[15488, 7744, 88, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[1, 1]](%input.175, %head.6.weight, %head.6.bias) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:443:0\n  %input.179 : Float(*, *, strides=[100, 1], requires_grad=1, device=cuda:0) = onnx::Gemm[alpha=1., beta=1., transB=1](%onnx::Gemm_148, %domain_classifier.0.weight, %domain_classifier.0.bias) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py:103:0\n  %input.183 : Float(*, *, strides=[100, 1], requires_grad=1, device=cuda:0) = onnx::BatchNormalization[epsilon=1.0000000000000001e-05, momentum=0.90000000000000002](%input.179, %domain_classifier.1.weight, %domain_classifier.1.bias, %domain_classifier.1.running_mean, %domain_classifier.1.running_var) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/functional.py:2421:0\n  %onnx::Gemm_178 : Float(*, *, strides=[100, 1], requires_grad=1, device=cuda:0) = onnx::Relu(%input.183) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/functional.py:1440:0\n  %input.187 : Float(*, *, strides=[2, 1], requires_grad=1, device=cuda:0) = onnx::Gemm[alpha=1., beta=1., transB=1](%onnx::Gemm_178, %domain_classifier.3.weight, %domain_classifier.3.bias) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py:103:0\n  %180 : Float(*, *, strides=[2, 1], requires_grad=1, device=cuda:0) = onnx::LogSoftmax[axis=1](%input.187) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/functional.py:1907:0\n  return (%output, %180)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [56]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01monnx\u001b[39;00m\n\u001b[1;32m      3\u001b[0m batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m720\u001b[39m,\u001b[38;5;241m720\u001b[39m)\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                 \u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1.\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                 \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDASemantic.onnx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mexport_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                 \u001b[49m\u001b[43minput_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                 \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moutput\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mopset_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m11\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/onnx/__init__.py:305\u001b[0m, in \u001b[0;36mexport\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;03mExports a model into ONNX format. If ``model`` is not a\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;124;03m:class:`torch.jit.ScriptModule` nor a :class:`torch.jit.ScriptFunction`, this runs\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;124;03m    model to the file ``f`` even if this is raised.\u001b[39;00m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01monnx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils\n\u001b[0;32m--> 305\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexport_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m                    \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopset_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mdo_constant_folding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_opsets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/onnx/utils.py:118\u001b[0m, in \u001b[0;36mexport\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m         operator_export_type \u001b[38;5;241m=\u001b[39m OperatorExportTypes\u001b[38;5;241m.\u001b[39mONNX\n\u001b[0;32m--> 118\u001b[0m \u001b[43m_export\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexport_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopset_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopset_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_constant_folding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_constant_folding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_opsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_opsets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/onnx/utils.py:743\u001b[0m, in \u001b[0;36m_export\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions)\u001b[0m\n\u001b[1;32m    738\u001b[0m     proto, export_map, val_use_external_data_format \u001b[38;5;241m=\u001b[39m graph\u001b[38;5;241m.\u001b[39m_export_onnx(\n\u001b[1;32m    739\u001b[0m         params_dict, opset_version, dynamic_axes, defer_weight_export,\n\u001b[1;32m    740\u001b[0m         operator_export_type, \u001b[38;5;129;01mnot\u001b[39;00m verbose, val_keep_init_as_ip, custom_opsets,\n\u001b[1;32m    741\u001b[0m         val_add_node_names, model_file_location, node_attr_to_name)\n\u001b[1;32m    742\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 743\u001b[0m     proto, export_map, val_use_external_data_format \u001b[38;5;241m=\u001b[39m \u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_export_onnx\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    744\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopset_version\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_keep_init_as_ip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_opsets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_add_node_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_file_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_attr_to_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m export_type \u001b[38;5;241m==\u001b[39m ExportTypes\u001b[38;5;241m.\u001b[39mPROTOBUF_FILE:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m(\u001b[38;5;28mlen\u001b[39m(export_map) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: ONNX export failed: Couldn't export Python operator GradientReversalFn\n\nDefined at:\n/tmp/ipykernel_914631/3131590279.py(125): forward\n/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py(1098): _slow_forward\n/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py(1110): _call_impl\n/home/ubuntu/.local/lib/python3.8/site-packages/torch/jit/_trace.py(118): wrapper\n/home/ubuntu/.local/lib/python3.8/site-packages/torch/jit/_trace.py(127): forward\n/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py(1110): _call_impl\n/home/ubuntu/.local/lib/python3.8/site-packages/torch/jit/_trace.py(1166): _get_trace_graph\n/home/ubuntu/.local/lib/python3.8/site-packages/torch/onnx/utils.py(391): _trace_and_get_graph_from_model\n/home/ubuntu/.local/lib/python3.8/site-packages/torch/onnx/utils.py(440): _create_jit_graph\n/home/ubuntu/.local/lib/python3.8/site-packages/torch/onnx/utils.py(499): _model_to_graph\n/home/ubuntu/.local/lib/python3.8/site-packages/torch/onnx/utils.py(719): _export\n/home/ubuntu/.local/lib/python3.8/site-packages/torch/onnx/utils.py(118): export\n/home/ubuntu/.local/lib/python3.8/site-packages/torch/onnx/__init__.py(305): export\n/tmp/ipykernel_914631/343856551.py(5): <module>\n/home/ubuntu/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py(3251): run_code\n/home/ubuntu/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py(3191): run_ast_nodes\n/home/ubuntu/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py(3012): run_cell_async\n/home/ubuntu/.local/lib/python3.8/site-packages/IPython/core/async_helpers.py(129): _pseudo_sync_runner\n/home/ubuntu/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py(2814): _run_cell\n/home/ubuntu/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py(2768): run_cell\n/home/ubuntu/.local/lib/python3.8/site-packages/ipykernel/zmqshell.py(532): run_cell\n/home/ubuntu/.local/lib/python3.8/site-packages/ipykernel/ipkernel.py(353): do_execute\n/home/ubuntu/.local/lib/python3.8/site-packages/ipykernel/kernelbase.py(652): execute_request\n/home/ubuntu/.local/lib/python3.8/site-packages/ipykernel/kernelbase.py(357): dispatch_shell\n/home/ubuntu/.local/lib/python3.8/site-packages/ipykernel/kernelbase.py(450): process_one\n/home/ubuntu/.local/lib/python3.8/site-packages/ipykernel/kernelbase.py(461): dispatch_queue\n/usr/lib/python3.8/asyncio/events.py(81): _run\n/usr/lib/python3.8/asyncio/base_events.py(1859): _run_once\n/usr/lib/python3.8/asyncio/base_events.py(570): run_forever\n/home/ubuntu/.local/lib/python3.8/site-packages/tornado/platform/asyncio.py(199): start\n/home/ubuntu/.local/lib/python3.8/site-packages/ipykernel/kernelapp.py(677): start\n/home/ubuntu/.local/lib/python3.8/site-packages/traitlets/config/application.py(846): launch_instance\n/home/ubuntu/.local/lib/python3.8/site-packages/ipykernel_launcher.py(16): <module>\n/usr/lib/python3.8/runpy.py(87): _run_code\n/usr/lib/python3.8/runpy.py(194): _run_module_as_main\n\n\nGraph we tried to export:\ngraph(%input : Float(2, 3, 720, 720, strides=[1555200, 518400, 720, 1], requires_grad=0, device=cuda:0),\n      %prim::PythonOp_1 : Double(requires_grad=0, device=cpu),\n      %block1.1.depthwise.weight : Float(32, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=1, device=cuda:0),\n      %block1.4.depthwise.weight : Float(32, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=1, device=cuda:0),\n      %residual1.weight : Float(32, 32, 1, 1, strides=[32, 1, 1, 1], requires_grad=1, device=cuda:0),\n      %residual1.bias : Float(32, strides=[1], requires_grad=1, device=cuda:0),\n      %block2.1.depthwise.weight : Float(32, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=1, device=cuda:0),\n      %block2.4.depthwise.weight : Float(64, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=1, device=cuda:0),\n      %residual2.weight : Float(64, 32, 1, 1, strides=[32, 1, 1, 1], requires_grad=1, device=cuda:0),\n      %residual2.bias : Float(64, strides=[1], requires_grad=1, device=cuda:0),\n      %block3.1.depthwise.weight : Float(64, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=1, device=cuda:0),\n      %block3.4.depthwise.weight : Float(128, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=1, device=cuda:0),\n      %residual3.weight : Float(128, 64, 1, 1, strides=[64, 1, 1, 1], requires_grad=1, device=cuda:0),\n      %residual3.bias : Float(128, strides=[1], requires_grad=1, device=cuda:0),\n      %domain_classifier.0.weight : Float(100, 259200, strides=[259200, 1], requires_grad=1, device=cuda:0),\n      %domain_classifier.0.bias : Float(100, strides=[1], requires_grad=1, device=cuda:0),\n      %domain_classifier.1.weight : Float(100, strides=[1], requires_grad=1, device=cuda:0),\n      %domain_classifier.1.bias : Float(100, strides=[1], requires_grad=1, device=cuda:0),\n      %domain_classifier.1.running_mean : Float(100, strides=[1], requires_grad=0, device=cuda:0),\n      %domain_classifier.1.running_var : Float(100, strides=[1], requires_grad=0, device=cuda:0),\n      %domain_classifier.3.weight : Float(2, 100, strides=[100, 1], requires_grad=1, device=cuda:0),\n      %domain_classifier.3.bias : Float(2, strides=[1], requires_grad=1, device=cuda:0),\n      %upsample1.1.weight : Float(128, 128, 3, 3, strides=[1152, 9, 3, 1], requires_grad=1, device=cuda:0),\n      %upsample1.1.bias : Float(128, strides=[1], requires_grad=1, device=cuda:0),\n      %upsample1.2.weight : Float(128, strides=[1], requires_grad=1, device=cuda:0),\n      %upsample1.2.bias : Float(128, strides=[1], requires_grad=1, device=cuda:0),\n      %upsample1.2.running_mean : Float(128, strides=[1], requires_grad=0, device=cuda:0),\n      %upsample1.2.running_var : Float(128, strides=[1], requires_grad=0, device=cuda:0),\n      %upsample1.4.weight : Float(128, 128, 3, 3, strides=[1152, 9, 3, 1], requires_grad=1, device=cuda:0),\n      %upsample1.4.bias : Float(128, strides=[1], requires_grad=1, device=cuda:0),\n      %upsample1.5.weight : Float(128, strides=[1], requires_grad=1, device=cuda:0),\n      %upsample1.5.bias : Float(128, strides=[1], requires_grad=1, device=cuda:0),\n      %upsample1.5.running_mean : Float(128, strides=[1], requires_grad=0, device=cuda:0),\n      %upsample1.5.running_var : Float(128, strides=[1], requires_grad=0, device=cuda:0),\n      %residual4.1.weight : Float(128, 128, 1, 1, strides=[128, 1, 1, 1], requires_grad=1, device=cuda:0),\n      %residual4.1.bias : Float(128, strides=[1], requires_grad=1, device=cuda:0),\n      %head.1.depthwise.weight : Float(128, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=1, device=cuda:0),\n      %head.4.depthwise.weight : Float(64, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=1, device=cuda:0),\n      %head.6.weight : Float(2, 32, 3, 3, strides=[288, 9, 3, 1], requires_grad=1, device=cuda:0),\n      %head.6.bias : Float(2, strides=[1], requires_grad=1, device=cuda:0),\n      %onnx::Conv_182 : Float(32, 3, 3, 3, strides=[27, 9, 3, 1], requires_grad=0, device=cuda:0),\n      %onnx::Conv_183 : Float(32, strides=[1], requires_grad=0, device=cuda:0),\n      %onnx::Conv_185 : Float(32, 32, 1, 1, strides=[32, 1, 1, 1], requires_grad=0, device=cuda:0),\n      %onnx::Conv_186 : Float(32, strides=[1], requires_grad=0, device=cuda:0),\n      %onnx::Conv_188 : Float(32, 32, 1, 1, strides=[32, 1, 1, 1], requires_grad=0, device=cuda:0),\n      %onnx::Conv_189 : Float(32, strides=[1], requires_grad=0, device=cuda:0),\n      %onnx::Conv_191 : Float(64, 32, 1, 1, strides=[32, 1, 1, 1], requires_grad=0, device=cuda:0),\n      %onnx::Conv_192 : Float(64, strides=[1], requires_grad=0, device=cuda:0),\n      %onnx::Conv_194 : Float(64, 64, 1, 1, strides=[64, 1, 1, 1], requires_grad=0, device=cuda:0),\n      %onnx::Conv_195 : Float(64, strides=[1], requires_grad=0, device=cuda:0),\n      %onnx::Conv_197 : Float(128, 64, 1, 1, strides=[64, 1, 1, 1], requires_grad=0, device=cuda:0),\n      %onnx::Conv_198 : Float(128, strides=[1], requires_grad=0, device=cuda:0),\n      %onnx::Conv_200 : Float(128, 128, 1, 1, strides=[128, 1, 1, 1], requires_grad=0, device=cuda:0),\n      %onnx::Conv_201 : Float(128, strides=[1], requires_grad=0, device=cuda:0),\n      %onnx::Conv_203 : Float(64, 128, 1, 1, strides=[128, 1, 1, 1], requires_grad=0, device=cuda:0),\n      %onnx::Conv_204 : Float(64, strides=[1], requires_grad=0, device=cuda:0),\n      %onnx::Conv_206 : Float(32, 64, 1, 1, strides=[64, 1, 1, 1], requires_grad=0, device=cuda:0),\n      %onnx::Conv_207 : Float(32, strides=[1], requires_grad=0, device=cuda:0),\n      %onnx::ConstantOfShape_208 : Long(1, strides=[1], requires_grad=0, device=cpu),\n      %onnx::Resize_209 : Float(4, strides=[1], requires_grad=0, device=cpu),\n      %onnx::Resize_210 : Float(4, strides=[1], requires_grad=0, device=cpu)):\n  %onnx::Where_98 : Long(4, strides=[1], device=cpu) = onnx::Constant[value=   2    3  720  720 [ CPULongType{4} ]]() # /tmp/ipykernel_914631/3131590279.py:89:0\n  %onnx::Mul_100 : Long(4, device=cpu) = onnx::ConstantOfShape[value={1}](%onnx::ConstantOfShape_208) # /tmp/ipykernel_914631/3131590279.py:89:0\n  %onnx::Mul_101 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}]() # /tmp/ipykernel_914631/3131590279.py:89:0\n  %onnx::Equal_102 : Long(4, strides=[1], device=cpu) = onnx::Mul(%onnx::Mul_100, %onnx::Mul_101) # /tmp/ipykernel_914631/3131590279.py:89:0\n  %onnx::Equal_103 : Long(4, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value=   2    3  720  720 [ CPULongType{4} ]]() # /tmp/ipykernel_914631/3131590279.py:89:0\n  %onnx::Where_104 : Bool(4, strides=[1], device=cpu) = onnx::Equal(%onnx::Equal_103, %onnx::Equal_102) # /tmp/ipykernel_914631/3131590279.py:89:0\n  %onnx::Expand_105 : Long(4, strides=[1], device=cpu) = onnx::Where(%onnx::Where_104, %onnx::Mul_100, %onnx::Where_98) # /tmp/ipykernel_914631/3131590279.py:89:0\n  %input.1 : Float(2, 3, 720, 720, strides=[1555200, 518400, 720, 1], requires_grad=0, device=cuda:0) = onnx::Expand(%input, %onnx::Expand_105) # /tmp/ipykernel_914631/3131590279.py:89:0\n  %input.7 : Float(2, 32, 359, 359, strides=[4124192, 128881, 359, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2]](%input.1, %onnx::Conv_182, %onnx::Conv_183) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:443:0\n  %input.11 : Float(2, 32, 359, 359, strides=[4124192, 128881, 359, 1], requires_grad=1, device=cuda:0) = onnx::Relu(%input.7) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/functional.py:1442:0\n  %input.15 : Float(2, 32, 359, 359, strides=[4124192, 128881, 359, 1], requires_grad=1, device=cuda:0) = onnx::Relu(%input.11) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/functional.py:1442:0\n  %input.19 : Float(2, 32, 359, 359, strides=[4124192, 128881, 359, 1], requires_grad=0, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=32, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%input.15, %block1.1.depthwise.weight) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:443:0\n  %input.27 : Float(2, 32, 359, 359, strides=[4124192, 128881, 359, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%input.19, %onnx::Conv_185, %onnx::Conv_186) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:443:0\n  %input.31 : Float(2, 32, 359, 359, strides=[4124192, 128881, 359, 1], requires_grad=1, device=cuda:0) = onnx::Relu(%input.27) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/functional.py:1442:0\n  %input.35 : Float(2, 32, 359, 359, strides=[4124192, 128881, 359, 1], requires_grad=0, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=32, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%input.31, %block1.4.depthwise.weight) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:443:0\n  %input.43 : Float(2, 32, 359, 359, strides=[4124192, 128881, 359, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%input.35, %onnx::Conv_188, %onnx::Conv_189) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:443:0\n  %onnx::MaxPool_118 : Float(2, 32, 359, 359, strides=[4124192, 128881, 359, 1], requires_grad=1, device=cuda:0) = onnx::Relu(%input.43) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/functional.py:1442:0\n  %onnx::Add_119 : Float(2, 32, 180, 180, strides=[1036800, 32400, 180, 1], requires_grad=1, device=cuda:0) = onnx::MaxPool[ceil_mode=0, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%onnx::MaxPool_118) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/functional.py:797:0\n  %onnx::Add_120 : Float(2, 32, 180, 180, strides=[1036800, 32400, 180, 1], requires_grad=0, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[2, 2]](%input.11, %residual1.weight, %residual1.bias) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:443:0\n  %input.47 : Float(2, 32, 180, 180, strides=[1036800, 32400, 180, 1], requires_grad=1, device=cuda:0) = onnx::Add(%onnx::Add_119, %onnx::Add_120) # /tmp/ipykernel_914631/3131590279.py:102:0\n  %input.51 : Float(2, 32, 180, 180, strides=[1036800, 32400, 180, 1], requires_grad=1, device=cuda:0) = onnx::Relu(%input.47) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/functional.py:1442:0\n  %input.55 : Float(2, 32, 180, 180, strides=[1036800, 32400, 180, 1], requires_grad=0, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=32, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%input.51, %block2.1.depthwise.weight) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:443:0\n  %input.63 : Float(2, 64, 180, 180, strides=[2073600, 32400, 180, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%input.55, %onnx::Conv_191, %onnx::Conv_192) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:443:0\n  %input.67 : Float(2, 64, 180, 180, strides=[2073600, 32400, 180, 1], requires_grad=1, device=cuda:0) = onnx::Relu(%input.63) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/functional.py:1442:0\n  %input.71 : Float(2, 64, 180, 180, strides=[2073600, 32400, 180, 1], requires_grad=0, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=64, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%input.67, %block2.4.depthwise.weight) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:443:0\n  %input.79 : Float(2, 64, 180, 180, strides=[2073600, 32400, 180, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%input.71, %onnx::Conv_194, %onnx::Conv_195) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:443:0\n  %onnx::MaxPool_130 : Float(2, 64, 180, 180, strides=[2073600, 32400, 180, 1], requires_grad=1, device=cuda:0) = onnx::Relu(%input.79) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/functional.py:1442:0\n  %onnx::Add_131 : Float(2, 64, 90, 90, strides=[518400, 8100, 90, 1], requires_grad=1, device=cuda:0) = onnx::MaxPool[ceil_mode=0, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%onnx::MaxPool_130) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/functional.py:797:0\n  %onnx::Add_132 : Float(2, 64, 90, 90, strides=[518400, 8100, 90, 1], requires_grad=0, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[2, 2]](%input.47, %residual2.weight, %residual2.bias) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:443:0\n  %input.83 : Float(2, 64, 90, 90, strides=[518400, 8100, 90, 1], requires_grad=1, device=cuda:0) = onnx::Add(%onnx::Add_131, %onnx::Add_132) # /tmp/ipykernel_914631/3131590279.py:110:0\n  %input.87 : Float(2, 64, 90, 90, strides=[518400, 8100, 90, 1], requires_grad=1, device=cuda:0) = onnx::Relu(%input.83) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/functional.py:1442:0\n  %input.91 : Float(2, 64, 90, 90, strides=[518400, 8100, 90, 1], requires_grad=0, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=64, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%input.87, %block3.1.depthwise.weight) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:443:0\n  %input.99 : Float(2, 128, 90, 90, strides=[1036800, 8100, 90, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%input.91, %onnx::Conv_197, %onnx::Conv_198) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:443:0\n  %input.103 : Float(2, 128, 90, 90, strides=[1036800, 8100, 90, 1], requires_grad=1, device=cuda:0) = onnx::Relu(%input.99) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/functional.py:1442:0\n  %input.107 : Float(2, 128, 90, 90, strides=[1036800, 8100, 90, 1], requires_grad=0, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=128, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%input.103, %block3.4.depthwise.weight) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:443:0\n  %input.115 : Float(2, 128, 90, 90, strides=[1036800, 8100, 90, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%input.107, %onnx::Conv_200, %onnx::Conv_201) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:443:0\n  %onnx::MaxPool_142 : Float(2, 128, 90, 90, strides=[1036800, 8100, 90, 1], requires_grad=1, device=cuda:0) = onnx::Relu(%input.115) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/functional.py:1442:0\n  %onnx::Add_143 : Float(2, 128, 45, 45, strides=[259200, 2025, 45, 1], requires_grad=1, device=cuda:0) = onnx::MaxPool[ceil_mode=0, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%onnx::MaxPool_142) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/functional.py:797:0\n  %onnx::Add_144 : Float(2, 128, 45, 45, strides=[259200, 2025, 45, 1], requires_grad=0, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[2, 2]](%input.83, %residual3.weight, %residual3.bias) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:443:0\n  %input.119 : Float(2, 128, 45, 45, strides=[259200, 2025, 45, 1], requires_grad=1, device=cuda:0) = onnx::Add(%onnx::Add_143, %onnx::Add_144) # /tmp/ipykernel_914631/3131590279.py:118:0\n  %onnx::Reshape_146 : Long(2, strides=[1], device=cpu) = onnx::Constant[value=     -1  259200 [ CPULongType{2} ]]() # /tmp/ipykernel_914631/3131590279.py:123:0\n  %features : Float(2, 259200, strides=[259200, 1], requires_grad=1, device=cuda:0) = onnx::Reshape(%input.119, %onnx::Reshape_146) # /tmp/ipykernel_914631/3131590279.py:123:0\n  %onnx::Gemm_148 : Float(*, *, strides=[259200, 1], requires_grad=1, device=cuda:0) = ^GradientReversalFn()(%features, %prim::PythonOp_1) # /tmp/ipykernel_914631/3131590279.py:125:0\n  %onnx::ConvTranspose_149 : Float(2, 128, 45, 45, strides=[259200, 2025, 45, 1], requires_grad=1, device=cuda:0) = onnx::Relu(%input.119) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/functional.py:1442:0\n  %input.123 : Float(2, 128, 45, 45, strides=[259200, 2025, 45, 1], requires_grad=0, device=cuda:0) = onnx::ConvTranspose[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%onnx::ConvTranspose_149, %upsample1.1.weight, %upsample1.1.bias) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:925:0\n  %input.127 : Float(2, 128, 45, 45, strides=[259200, 2025, 45, 1], requires_grad=1, device=cuda:0) = onnx::BatchNormalization[epsilon=0.001, momentum=0.010000000000000009](%input.123, %upsample1.2.weight, %upsample1.2.bias, %upsample1.2.running_mean, %upsample1.2.running_var) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/functional.py:2421:0\n  %onnx::ConvTranspose_152 : Float(2, 128, 45, 45, strides=[259200, 2025, 45, 1], requires_grad=1, device=cuda:0) = onnx::Relu(%input.127) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/functional.py:1442:0\n  %input.131 : Float(2, 128, 45, 45, strides=[259200, 2025, 45, 1], requires_grad=0, device=cuda:0) = onnx::ConvTranspose[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%onnx::ConvTranspose_152, %upsample1.4.weight, %upsample1.4.bias) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:925:0\n  %input.135 : Float(2, 128, 45, 45, strides=[259200, 2025, 45, 1], requires_grad=1, device=cuda:0) = onnx::BatchNormalization[epsilon=0.001, momentum=0.010000000000000009](%input.131, %upsample1.5.weight, %upsample1.5.bias, %upsample1.5.running_mean, %upsample1.5.running_var) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/functional.py:2421:0\n  %onnx::Resize_158 : Float(0, strides=[1], device=cpu) = onnx::Constant[value=[ CPUFloatType{0} ]]() # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/functional.py:3891:0\n  %onnx::Add_159 : Float(2, 128, 90, 90, strides=[1036800, 8100, 90, 1], requires_grad=1, device=cuda:0) = onnx::Resize[coordinate_transformation_mode=\"asymmetric\", cubic_coeff_a=-0.75, mode=\"nearest\", nearest_mode=\"floor\"](%input.135, %onnx::Resize_158, %onnx::Resize_209) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/functional.py:3891:0\n  %onnx::Resize_163 : Float(0, strides=[1], device=cpu) = onnx::Constant[value=[ CPUFloatType{0} ]]() # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/functional.py:3891:0\n  %input.139 : Float(2, 128, 90, 90, strides=[1036800, 8100, 90, 1], requires_grad=1, device=cuda:0) = onnx::Resize[coordinate_transformation_mode=\"asymmetric\", cubic_coeff_a=-0.75, mode=\"nearest\", nearest_mode=\"floor\"](%input.119, %onnx::Resize_163, %onnx::Resize_210) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/functional.py:3891:0\n  %onnx::Add_165 : Float(2, 128, 90, 90, strides=[1036800, 8100, 90, 1], requires_grad=0, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%input.139, %residual4.1.weight, %residual4.1.bias) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:443:0\n  %input.143 : Float(2, 128, 90, 90, strides=[1036800, 8100, 90, 1], requires_grad=1, device=cuda:0) = onnx::Add(%onnx::Add_159, %onnx::Add_165) # /tmp/ipykernel_914631/3131590279.py:134:0\n  %input.147 : Float(2, 128, 90, 90, strides=[1036800, 8100, 90, 1], requires_grad=1, device=cuda:0) = onnx::Relu(%input.143) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/functional.py:1442:0\n  %input.151 : Float(2, 128, 90, 90, strides=[1036800, 8100, 90, 1], requires_grad=0, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=128, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%input.147, %head.1.depthwise.weight) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:443:0\n  %input.159 : Float(2, 64, 90, 90, strides=[518400, 8100, 90, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%input.151, %onnx::Conv_203, %onnx::Conv_204) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:443:0\n  %input.163 : Float(2, 64, 90, 90, strides=[518400, 8100, 90, 1], requires_grad=1, device=cuda:0) = onnx::Relu(%input.159) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/functional.py:1442:0\n  %input.167 : Float(2, 64, 90, 90, strides=[518400, 8100, 90, 1], requires_grad=0, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=64, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%input.163, %head.4.depthwise.weight) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:443:0\n  %input.175 : Float(2, 32, 90, 90, strides=[259200, 8100, 90, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%input.167, %onnx::Conv_206, %onnx::Conv_207) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:443:0\n  %output : Float(2, 2, 88, 88, strides=[15488, 7744, 88, 1], requires_grad=1, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[1, 1]](%input.175, %head.6.weight, %head.6.bias) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:443:0\n  %input.179 : Float(*, *, strides=[100, 1], requires_grad=1, device=cuda:0) = onnx::Gemm[alpha=1., beta=1., transB=1](%onnx::Gemm_148, %domain_classifier.0.weight, %domain_classifier.0.bias) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py:103:0\n  %input.183 : Float(*, *, strides=[100, 1], requires_grad=1, device=cuda:0) = onnx::BatchNormalization[epsilon=1.0000000000000001e-05, momentum=0.90000000000000002](%input.179, %domain_classifier.1.weight, %domain_classifier.1.bias, %domain_classifier.1.running_mean, %domain_classifier.1.running_var) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/functional.py:2421:0\n  %onnx::Gemm_178 : Float(*, *, strides=[100, 1], requires_grad=1, device=cuda:0) = onnx::Relu(%input.183) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/functional.py:1440:0\n  %input.187 : Float(*, *, strides=[2, 1], requires_grad=1, device=cuda:0) = onnx::Gemm[alpha=1., beta=1., transB=1](%onnx::Gemm_178, %domain_classifier.3.weight, %domain_classifier.3.bias) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py:103:0\n  %180 : Float(*, *, strides=[2, 1], requires_grad=1, device=cuda:0) = onnx::LogSoftmax[axis=1](%input.187) # /home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/functional.py:1907:0\n  return (%output, %180)\n"
     ]
    }
   ],
   "source": [
    "# Save as ONNX model won't work because they don't support \n",
    "# custom autograd functions\n",
    "\n",
    "import torch.onnx\n",
    "\n",
    "batch = torch.rand(2,3,720,720).cuda()\n",
    "\n",
    "torch.onnx.export(model,\n",
    "                 (batch,1.),\n",
    "                 \"DASemantic.onnx\",\n",
    "                 export_params=False,\n",
    "                 input_names=['input'],\n",
    "                 output_names=['output'],\n",
    "                 opset_version=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2f17d1",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "c36c3b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob as glob\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, img_dir, mask_dir, domain_label, img_transform=None, mask_transform=None):\n",
    "        self.img_dir = glob(os.path.join(img_dir,\"*\"))\n",
    "        self.mask_dir = [os.path.join(mask_dir,f\"mask{os.path.splitext(os.path.basename(img_name))[0][3:]}.jpg\") for img_name in glob(os.path.join(img_dir,\"*\"))]\n",
    "        self.domain_label = torch.ones(len(glob(os.path.join(img_dir,\"*\")))) if domain_label else torch.zeros(len(glob(os.path.join(img_dir,\"*\"))))\n",
    "        self.img_transform = img_transform\n",
    "        self.mask_transform = mask_transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_dir)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = cv.imread(self.img_dir[idx])\n",
    "        image = self.img_transform(image)\n",
    "        \n",
    "        mask = cv.imread(self.mask_dir[idx])\n",
    "        mask = self.mask_transform(mask)\n",
    "    \n",
    "        domain = self.domain_label[idx]\n",
    "    \n",
    "        return image, mask, domain\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "6f2aceda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Transform\n",
    "\n",
    "class Threshold(object):\n",
    "    def __init__(self, threshold):\n",
    "        assert isinstance(threshold, (int))\n",
    "        self.threshold = threshold\n",
    "        \n",
    "    def __call__(self, img):\n",
    "        img[img<=self.threshold] = 0\n",
    "        img[img>self.threshold] = 255\n",
    "        \n",
    "        return img\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "32a49e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[ 0.4516,  0.4186,  0.3509,  ...,  0.0027, -0.0640, -0.0827],\n",
      "          [ 0.4772,  0.4281,  0.4232,  ...,  0.0029, -0.0421, -0.0483],\n",
      "          [ 0.4602,  0.4163,  0.4906,  ...,  0.0045, -0.0473, -0.0312],\n",
      "          ...,\n",
      "          [ 1.9521,  1.9814,  1.9831,  ...,  0.4361,  0.5631,  0.5930],\n",
      "          [ 2.0022,  1.9722,  1.9453,  ...,  0.5736,  0.5870,  0.6008],\n",
      "          [ 1.9127,  1.9565,  2.0406,  ...,  0.6192,  0.6194,  0.5094]],\n",
      "\n",
      "         [[ 0.5911,  0.5574,  0.4882,  ...,  0.1322,  0.0640,  0.0449],\n",
      "          [ 0.6173,  0.5671,  0.5621,  ...,  0.1324,  0.0864,  0.0801],\n",
      "          [ 0.5999,  0.5551,  0.6311,  ...,  0.1340,  0.0811,  0.0976],\n",
      "          ...,\n",
      "          [ 2.1252,  2.1551,  2.1569,  ...,  0.5753,  0.7052,  0.7357],\n",
      "          [ 2.1763,  2.1456,  2.1182,  ...,  0.7159,  0.7296,  0.7437],\n",
      "          [ 2.0848,  2.1296,  2.2156,  ...,  0.7625,  0.7627,  0.6502]],\n",
      "\n",
      "         [[ 0.8107,  0.7771,  0.7083,  ...,  0.3539,  0.2860,  0.2669],\n",
      "          [ 0.8368,  0.7868,  0.7819,  ...,  0.3540,  0.3082,  0.3020],\n",
      "          [ 0.8195,  0.7748,  0.8505,  ...,  0.3557,  0.3029,  0.3194],\n",
      "          ...,\n",
      "          [ 2.3379,  2.3678,  2.3695,  ...,  0.7950,  0.9243,  0.9547],\n",
      "          [ 2.3889,  2.3583,  2.3310,  ...,  0.9349,  0.9486,  0.9626],\n",
      "          [ 2.2978,  2.3424,  2.4280,  ...,  0.9813,  0.9816,  0.8695]]],\n",
      "\n",
      "\n",
      "        [[[ 1.5906,  1.7466,  1.6943,  ...,  2.1680,  2.2061,  2.1338],\n",
      "          [ 1.6686,  1.7169,  1.6529,  ...,  2.1908,  2.1939,  2.1723],\n",
      "          [ 1.7116,  1.6950,  1.6506,  ...,  2.2102,  2.1962,  2.2087],\n",
      "          ...,\n",
      "          [-0.1559, -0.1798, -0.2070,  ..., -0.1655, -0.1365, -0.0870],\n",
      "          [-0.1690, -0.1812, -0.2068,  ..., -0.1188, -0.0454, -0.0881],\n",
      "          [-0.1999, -0.2056, -0.2342,  ..., -0.1771, -0.0629, -0.2075]],\n",
      "\n",
      "         [[ 1.7555,  1.9150,  1.8615,  ...,  2.3459,  2.3848,  2.3109],\n",
      "          [ 1.8354,  1.8847,  1.8192,  ...,  2.3691,  2.3723,  2.3502],\n",
      "          [ 1.8793,  1.8623,  1.8169,  ...,  2.3890,  2.3747,  2.3875],\n",
      "          ...,\n",
      "          [-0.0299, -0.0544, -0.0821,  ..., -0.0398, -0.0101,  0.0405],\n",
      "          [-0.0434, -0.0557, -0.0819,  ...,  0.0080,  0.0830,  0.0394],\n",
      "          [-0.0749, -0.0808, -0.1099,  ..., -0.0516,  0.0651, -0.0827]],\n",
      "\n",
      "         [[ 1.9699,  2.1287,  2.0755,  ...,  2.5577,  2.5964,  2.5228],\n",
      "          [ 2.0494,  2.0985,  2.0334,  ...,  2.5808,  2.5840,  2.5620],\n",
      "          [ 2.0932,  2.0763,  2.0311,  ...,  2.6006,  2.5864,  2.5991],\n",
      "          ...,\n",
      "          [ 0.1924,  0.1681,  0.1404,  ...,  0.1826,  0.2122,  0.2626],\n",
      "          [ 0.1791,  0.1667,  0.1406,  ...,  0.2302,  0.3049,  0.2615],\n",
      "          [ 0.1476,  0.1418,  0.1128,  ...,  0.1709,  0.2871,  0.1399]]],\n",
      "\n",
      "\n",
      "        [[[-0.6794, -0.6844, -0.6992,  ...,  2.1186,  2.1776,  2.1975],\n",
      "          [-0.6794, -0.6844, -0.6992,  ...,  2.1186,  2.1776,  2.1975],\n",
      "          [-0.6794, -0.6852, -0.7022,  ...,  2.1102,  2.1715,  2.1922],\n",
      "          ...,\n",
      "          [ 0.5299,  0.5312,  0.5351,  ..., -0.0082, -0.0156, -0.0181],\n",
      "          [ 0.5193,  0.5268,  0.5489,  ..., -0.0188, -0.0262, -0.0287],\n",
      "          [ 0.5193,  0.5268,  0.5489,  ..., -0.0188, -0.0262, -0.0287]],\n",
      "\n",
      "         [[-0.5651, -0.5702, -0.5853,  ...,  2.2953,  2.3556,  2.3761],\n",
      "          [-0.5651, -0.5702, -0.5853,  ...,  2.2953,  2.3556,  2.3761],\n",
      "          [-0.5651, -0.5710, -0.5884,  ...,  2.2868,  2.3494,  2.3706],\n",
      "          ...,\n",
      "          [ 0.6712,  0.6725,  0.6765,  ...,  0.1211,  0.1135,  0.1110],\n",
      "          [ 0.6604,  0.6680,  0.6906,  ...,  0.1102,  0.1027,  0.1001],\n",
      "          [ 0.6604,  0.6680,  0.6906,  ...,  0.1102,  0.1027,  0.1001]],\n",
      "\n",
      "         [[-0.3404, -0.3455, -0.3605,  ...,  2.5073,  2.5674,  2.5877],\n",
      "          [-0.3404, -0.3455, -0.3605,  ...,  2.5073,  2.5674,  2.5877],\n",
      "          [-0.3404, -0.3463, -0.3636,  ...,  2.4989,  2.5612,  2.5823],\n",
      "          ...,\n",
      "          [ 0.8904,  0.8918,  0.8957,  ...,  0.3427,  0.3352,  0.3327],\n",
      "          [ 0.8797,  0.8873,  0.9098,  ...,  0.3320,  0.3245,  0.3219],\n",
      "          [ 0.8797,  0.8873,  0.9098,  ...,  0.3320,  0.3245,  0.3219]]],\n",
      "\n",
      "\n",
      "        [[[-1.9295, -1.9295, -1.9295,  ..., -1.8782, -1.8782, -1.8782],\n",
      "          [-1.9295, -1.9295, -1.9295,  ..., -1.8782, -1.8782, -1.8782],\n",
      "          [-1.9295, -1.9295, -1.9295,  ..., -1.8782, -1.8782, -1.8782],\n",
      "          ...,\n",
      "          [-1.7925, -1.7925, -1.7925,  ..., -1.7583, -1.7583, -1.7583],\n",
      "          [-1.7925, -1.7925, -1.7925,  ..., -1.7583, -1.7583, -1.7583],\n",
      "          [-1.7925, -1.7925, -1.7925,  ..., -1.7583, -1.7583, -1.7583]],\n",
      "\n",
      "         [[-1.8431, -1.8431, -1.8431,  ..., -1.7906, -1.7906, -1.7906],\n",
      "          [-1.8431, -1.8431, -1.8431,  ..., -1.7906, -1.7906, -1.7906],\n",
      "          [-1.8431, -1.8431, -1.8431,  ..., -1.7906, -1.7906, -1.7906],\n",
      "          ...,\n",
      "          [-1.7031, -1.7031, -1.7031,  ..., -1.6681, -1.6681, -1.6681],\n",
      "          [-1.7031, -1.7031, -1.7031,  ..., -1.6681, -1.6681, -1.6681],\n",
      "          [-1.7031, -1.7031, -1.7031,  ..., -1.6681, -1.6681, -1.6681]],\n",
      "\n",
      "         [[-1.6127, -1.6127, -1.6127,  ..., -1.5604, -1.5604, -1.5604],\n",
      "          [-1.6127, -1.6127, -1.6127,  ..., -1.5604, -1.5604, -1.5604],\n",
      "          [-1.6127, -1.6127, -1.6127,  ..., -1.5604, -1.5604, -1.5604],\n",
      "          ...,\n",
      "          [-1.4733, -1.4733, -1.4733,  ..., -1.4384, -1.4384, -1.4384],\n",
      "          [-1.4733, -1.4733, -1.4733,  ..., -1.4384, -1.4384, -1.4384],\n",
      "          [-1.4733, -1.4733, -1.4733,  ..., -1.4384, -1.4384, -1.4384]]]]), tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]]), tensor([0., 0., 0., 0.])]\n"
     ]
    }
   ],
   "source": [
    "im_dir = '/home/ubuntu/workspace/create_train_set/data/dataset_out_mask/images/train'\n",
    "mask_dir = '/home/ubuntu/workspace/create_train_set/data/dataset_out_mask/masks/train'\n",
    "\n",
    "transform_img = transforms.Compose(\n",
    "    [\n",
    "        T.ToTensor(),\n",
    "        T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "        T.Resize((720,720))\n",
    "    ]\n",
    ")\n",
    "\n",
    "transform_mask = transforms.Compose(\n",
    "    [\n",
    "        Threshold(30),\n",
    "        T.ToTensor(),\n",
    "        #T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "        T.Resize((88,88))\n",
    "    ]\n",
    ")\n",
    "\n",
    "ds_source = CustomImageDataset(im_dir, mask_dir, 0, transform_img, transform_mask)\n",
    "\n",
    "\n",
    "dl_source = DataLoader(VirtualDataset, batch_size=4, shuffle=True)\n",
    "\n",
    "print(next(iter(dl_source)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cc77da",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "76b8e96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "n_epochs = 1\n",
    "\n",
    "# Setup optimizer\n",
    "model = DASemantic(720, 2).cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr)\n",
    "\n",
    "# Two Loss Functions (pixelwise class and domain)\n",
    "loss_fn_class = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "loss_fn_domain = nn.NLLLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7252d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# get data\n",
    "\n",
    "# find number of batches to run for\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a836a940",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "for epoch_idx in range(n_epochs):\n",
    "    print(f'Epoch {epoch_idx+1:04d} / {n_epochs:04d}', end='\\n=================\\n')\n",
    "    # source data iter\n",
    "    # target data iter\n",
    "    \n",
    "    for batch_idx in range(max_batches):\n",
    "        p = float(((batch_idx + epoch_idx * max_batches) / (n_epochs * max_batches))/2)\n",
    "        grl_lambda = 2. / (1. + np.exp(-10*p)) - 1\n",
    "        \n",
    "        # Train on source domain\n",
    "        X_s, y_s = next(dl_source_iter)\n",
    "        X_s, y_s = X_s.cuda(), y_s.cuda()\n",
    "        y_s_domain = torch.zeros(batch_size, dtype=torch.long) # generate source domain labels\n",
    "        y_s_domain = y_s_domain.cuda()\n",
    "        \n",
    "        class_pred, domain_pred = model(X_s, grl_lambda)\n",
    "        loss_s_label = loss_fn_class(class_pred, y_s)\n",
    "        loss_s_domain = loss_fn_domain(domain_pred, y_s_domain)\n",
    "        \n",
    "        # Train on target domain\n",
    "        X_t, _ = next(dl_target_iter) # ignore target domain class labels!\n",
    "        X_t = X_t.cuda()\n",
    "        y_t_domain = torch.ones(batch_size, dtype=torch.long) # generate target domain labels\n",
    "        y_t_domain = y_t_domain.cuda()\n",
    "        \n",
    "        _, domain_pred = model(X_t, grl_lambda)\n",
    "        loss_t_domain = loss_fn_domain(domain_pred, y_t_domain)\n",
    "        \n",
    "        # Calculate total loss\n",
    "        loss = loss_t_domain + loss_s_domain + loss_s_label\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f'[{batch_idx+1}/{max_batches}] '\n",
    "                  f'class_loss: {loss_s_label.item():.4f} ' f's_domain_loss: {loss_s_domain.item():.4f} '\n",
    "                  f't_domain_loss: {loss_t_domain.item():.4f} ' f'grl_lambda: {grl_lambda:.3f} '\n",
    "                 )\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c05cd1a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[35.4760, 99.9222],\n",
      "        [43.7156, 72.4414]])  +  tensor([[59.5805, 27.1421],\n",
      "        [19.7191, 31.2870]])\n",
      "tensor([[ 95.0565, 127.0644],\n",
      "        [ 63.4347, 103.7284]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(2,2)*100\n",
    "b = torch.rand(2,2)*100\n",
    "c = torch.add(a,b)\n",
    "print(a, \" + \", b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2fbe35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232aafb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6e313d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1181cdb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2c07bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bac1263",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494905e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
