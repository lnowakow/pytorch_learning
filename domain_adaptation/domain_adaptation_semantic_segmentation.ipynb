{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e158b210",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "root = './data'\n",
    "if not os.path.exists(root):\n",
    "    os.mkdir(root)\n",
    "    \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0ff213",
   "metadata": {},
   "source": [
    "### Gradient Reversal Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17e14463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient reversal\n",
    "from torch.autograd import Function\n",
    "\n",
    "# Autograd Function objects are what record operation history on tensors\n",
    "# and define formulas for the forwawrd and backprop.\n",
    "\n",
    "class GradientReversalFn(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):\n",
    "        # Store context for backprop\n",
    "        ctx.alpha = alpha\n",
    "        \n",
    "        # Forward pass is a no-op\n",
    "        return x.view_as(x)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # Backward pass is just to -alpha the gradient\n",
    "        output = grad_output.neg() * ctx.alpha\n",
    "        \n",
    "        # Must return same number as inputs to forward()\n",
    "        return output, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613ff8ee",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "77089fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 88, 88])\n"
     ]
    }
   ],
   "source": [
    "class SeparableConv2d(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, bias=False):\n",
    "        super(SeparableConv2d, self).__init__()\n",
    "        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size=kernel_size, \n",
    "                                   groups=in_channels, bias=bias, padding=1)\n",
    "        self.pointwise = nn.Conv2d(in_channels, out_channels, \n",
    "                                   kernel_size=1, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.depthwise(x)\n",
    "        out = self.pointwise(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class DASemantic(nn.Module):\n",
    "    def __init__(self, img_size, num_classes):\n",
    "        super(DASemantic, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.entry = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, stride=2),\n",
    "            nn.BatchNorm2d(num_features=32, eps=1e-3, momentum=0.99),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            SeparableConv2d(32, 32, 3),\n",
    "            nn.BatchNorm2d(num_features=32, eps=1e-3, momentum=0.99),\n",
    "            nn.ReLU(),\n",
    "            SeparableConv2d(32, 32, 3),\n",
    "            nn.BatchNorm2d(num_features=32, eps=1e-3, momentum=0.99),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3, stride=2, padding=1)\n",
    "        )\n",
    "        self.residual1 = nn.Conv2d(32, 32, 1, stride=2)\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            SeparableConv2d(32, 64, 3),\n",
    "            nn.BatchNorm2d(num_features=64, eps=1e-3, momentum=0.99),\n",
    "            nn.ReLU(),\n",
    "            SeparableConv2d(64, 64, 3),\n",
    "            nn.BatchNorm2d(num_features=64, eps=1e-3, momentum=0.99),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3, stride=2, padding=1)\n",
    "        )\n",
    "        self.residual2 = nn.Conv2d(32, 64, 1, stride=2)\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            SeparableConv2d(64, 128, 3),\n",
    "            nn.BatchNorm2d(num_features=128, eps=1e-3, momentum=0.99),\n",
    "            nn.ReLU(),\n",
    "            SeparableConv2d(128, 128, 3),\n",
    "            nn.BatchNorm2d(num_features=128, eps=1e-3, momentum=0.99),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3, stride=2, padding=1)\n",
    "        )\n",
    "        self.residual3 = nn.Conv2d(64, 128, 1, stride=2)\n",
    "        self.domain_classifier = nn.Sequential(\n",
    "            nn.Linear(128*45*45, 100), nn.BatchNorm1d(100),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(100,2),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "        self.upsample1 = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(num_features=128, eps=1e-3, momentum=0.99),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(num_features=128, eps=1e-3, momentum=0.99),\n",
    "            nn.UpsamplingNearest2d(scale_factor=2)\n",
    "        )\n",
    "        self.residual4 = nn.Sequential(\n",
    "            nn.UpsamplingNearest2d(scale_factor=2),\n",
    "            nn.Conv2d(128,128,1)\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            SeparableConv2d(128, 64, 3),\n",
    "            nn.BatchNorm2d(num_features=64, eps=1e-3, momentum=0.99),\n",
    "            nn.ReLU(),\n",
    "            SeparableConv2d(64, 32, 3),\n",
    "            nn.BatchNorm2d(num_features=32, eps=1e-3, momentum=0.99),\n",
    "            nn.Conv2d(32, num_classes, 3)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, grl_lambda):\n",
    "        # Handle single-channel input by expanding singleton dim\n",
    "        x = x.expand(x.data.shape[0], 3, self.img_size, self.img_size)\n",
    "        \n",
    "        # Entry block\n",
    "        x = self.entry(x)\n",
    "        previous_block_activation = x\n",
    "        \n",
    "        # Feature Depth\n",
    "        # block 1\n",
    "        x = self.block1(x)\n",
    "        #print('Post Block1: ' + str(x.shape))\n",
    "        previous_block_activation = self.residual1(previous_block_activation)\n",
    "        #print('Post Residual1: ' + str(previous_block_activation.shape))\n",
    "        x = torch.add(x, previous_block_activation)\n",
    "        #print('Post add1: ' + str(x.shape))\n",
    "        previous_block_activation = x\n",
    "        # block 2\n",
    "        x = self.block2(x)\n",
    "        #print('Post Block2: ' + str(x.shape))\n",
    "        previous_block_activation = self.residual2(previous_block_activation)\n",
    "        #print('Post Residual2: ' + str(previous_block_activation.shape))\n",
    "        x = torch.add(x, previous_block_activation)\n",
    "        #print('Post add2: ' + str(x.shape))\n",
    "        previous_block_activation = x\n",
    "        # block 3\n",
    "        x = self.block3(x)\n",
    "        #print('Post Block3: ' + str(x.shape))\n",
    "        previous_block_activation = self.residual3(previous_block_activation)\n",
    "        #print('Post Residual3: ' + str(previous_block_activation.shape))\n",
    "        x = torch.add(x, previous_block_activation)\n",
    "        #print('Post add3: ' + str(x.shape))\n",
    "        previous_block_activation = x\n",
    "        \n",
    "        # Gradient Reversal\n",
    "        features = x.view(-1, 128 * 45 * 45)\n",
    "        #print('GRL Feature vector: ' + str(features.shape))\n",
    "        reverse_features = GradientReversalFn.apply(features, grl_lambda)\n",
    "        #print('GRL Reverse Feature vector: ' + str(reverse_features.shape))\n",
    "        \n",
    "        # Upsampling\n",
    "        # upsample 1\n",
    "        x = self.upsample1(x)\n",
    "        #print('Post UpSample1: ' + str(x.shape))\n",
    "        previous_block_activation = self.residual4(previous_block_activation)\n",
    "        #print('Post Residual4: ' + str(previous_block_activation.shape))\n",
    "        x = torch.add(x, previous_block_activation)\n",
    "        #print('Post add4: ' + str(x.shape))\n",
    "        \n",
    "        # Head\n",
    "        output = self.head(x)\n",
    "        #print('output: ' + str(output.shape))\n",
    "        # Domain Pred\n",
    "        domain_pred = self.domain_classifier(reverse_features)\n",
    "        #print('domain_pred: ' + str(domain_pred.shape))\n",
    "        \n",
    "        return output, domain_pred\n",
    "\n",
    "model = DASemantic(720, 2).cuda()\n",
    "batch = torch.rand(2,3,720,720).cuda()\n",
    "print(model(batch, 1.)[0].shape)\n",
    "\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2f17d1",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "a23bf9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob as glob\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, img_dir, mask_dir, domain_label, img_transform=None, mask_transform=None):\n",
    "        self.img_dir = glob(os.path.join(img_dir,\"*\"))\n",
    "        self.mask_dir = [os.path.join(mask_dir,f\"mask{os.path.splitext(os.path.basename(img_name))[0][3:]}.jpg\") for img_name in glob(os.path.join(img_dir,\"*\"))]\n",
    "        self.domain_label = torch.ones(len(glob(os.path.join(img_dir,\"*\")))) if domain_label else torch.zeros(len(glob(os.path.join(img_dir,\"*\"))))\n",
    "        self.img_transform = img_transform\n",
    "        self.mask_transform = mask_transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_dir)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = cv.imread(self.img_dir[idx])\n",
    "        image = self.img_transform(image)\n",
    "        \n",
    "        mask = cv.imread(self.mask_dir[idx])\n",
    "        mask = self.mask_transform(mask)\n",
    "    \n",
    "        domain = self.domain_label[idx]\n",
    "    \n",
    "        return image, mask, domain\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "44964a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Transform\n",
    "\n",
    "class Threshold(object):\n",
    "    def __init__(self, threshold):\n",
    "        assert isinstance(threshold, (int))\n",
    "        self.threshold = threshold\n",
    "        \n",
    "    def __call__(self, img):\n",
    "        img[img<=self.threshold] = 0\n",
    "        img[img>self.threshold] = 255\n",
    "        \n",
    "        return img\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "4c0b5e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[ 0.4516,  0.4186,  0.3509,  ...,  0.0027, -0.0640, -0.0827],\n",
      "          [ 0.4772,  0.4281,  0.4232,  ...,  0.0029, -0.0421, -0.0483],\n",
      "          [ 0.4602,  0.4163,  0.4906,  ...,  0.0045, -0.0473, -0.0312],\n",
      "          ...,\n",
      "          [ 1.9521,  1.9814,  1.9831,  ...,  0.4361,  0.5631,  0.5930],\n",
      "          [ 2.0022,  1.9722,  1.9453,  ...,  0.5736,  0.5870,  0.6008],\n",
      "          [ 1.9127,  1.9565,  2.0406,  ...,  0.6192,  0.6194,  0.5094]],\n",
      "\n",
      "         [[ 0.5911,  0.5574,  0.4882,  ...,  0.1322,  0.0640,  0.0449],\n",
      "          [ 0.6173,  0.5671,  0.5621,  ...,  0.1324,  0.0864,  0.0801],\n",
      "          [ 0.5999,  0.5551,  0.6311,  ...,  0.1340,  0.0811,  0.0976],\n",
      "          ...,\n",
      "          [ 2.1252,  2.1551,  2.1569,  ...,  0.5753,  0.7052,  0.7357],\n",
      "          [ 2.1763,  2.1456,  2.1182,  ...,  0.7159,  0.7296,  0.7437],\n",
      "          [ 2.0848,  2.1296,  2.2156,  ...,  0.7625,  0.7627,  0.6502]],\n",
      "\n",
      "         [[ 0.8107,  0.7771,  0.7083,  ...,  0.3539,  0.2860,  0.2669],\n",
      "          [ 0.8368,  0.7868,  0.7819,  ...,  0.3540,  0.3082,  0.3020],\n",
      "          [ 0.8195,  0.7748,  0.8505,  ...,  0.3557,  0.3029,  0.3194],\n",
      "          ...,\n",
      "          [ 2.3379,  2.3678,  2.3695,  ...,  0.7950,  0.9243,  0.9547],\n",
      "          [ 2.3889,  2.3583,  2.3310,  ...,  0.9349,  0.9486,  0.9626],\n",
      "          [ 2.2978,  2.3424,  2.4280,  ...,  0.9813,  0.9816,  0.8695]]],\n",
      "\n",
      "\n",
      "        [[[ 1.5906,  1.7466,  1.6943,  ...,  2.1680,  2.2061,  2.1338],\n",
      "          [ 1.6686,  1.7169,  1.6529,  ...,  2.1908,  2.1939,  2.1723],\n",
      "          [ 1.7116,  1.6950,  1.6506,  ...,  2.2102,  2.1962,  2.2087],\n",
      "          ...,\n",
      "          [-0.1559, -0.1798, -0.2070,  ..., -0.1655, -0.1365, -0.0870],\n",
      "          [-0.1690, -0.1812, -0.2068,  ..., -0.1188, -0.0454, -0.0881],\n",
      "          [-0.1999, -0.2056, -0.2342,  ..., -0.1771, -0.0629, -0.2075]],\n",
      "\n",
      "         [[ 1.7555,  1.9150,  1.8615,  ...,  2.3459,  2.3848,  2.3109],\n",
      "          [ 1.8354,  1.8847,  1.8192,  ...,  2.3691,  2.3723,  2.3502],\n",
      "          [ 1.8793,  1.8623,  1.8169,  ...,  2.3890,  2.3747,  2.3875],\n",
      "          ...,\n",
      "          [-0.0299, -0.0544, -0.0821,  ..., -0.0398, -0.0101,  0.0405],\n",
      "          [-0.0434, -0.0557, -0.0819,  ...,  0.0080,  0.0830,  0.0394],\n",
      "          [-0.0749, -0.0808, -0.1099,  ..., -0.0516,  0.0651, -0.0827]],\n",
      "\n",
      "         [[ 1.9699,  2.1287,  2.0755,  ...,  2.5577,  2.5964,  2.5228],\n",
      "          [ 2.0494,  2.0985,  2.0334,  ...,  2.5808,  2.5840,  2.5620],\n",
      "          [ 2.0932,  2.0763,  2.0311,  ...,  2.6006,  2.5864,  2.5991],\n",
      "          ...,\n",
      "          [ 0.1924,  0.1681,  0.1404,  ...,  0.1826,  0.2122,  0.2626],\n",
      "          [ 0.1791,  0.1667,  0.1406,  ...,  0.2302,  0.3049,  0.2615],\n",
      "          [ 0.1476,  0.1418,  0.1128,  ...,  0.1709,  0.2871,  0.1399]]],\n",
      "\n",
      "\n",
      "        [[[-0.6794, -0.6844, -0.6992,  ...,  2.1186,  2.1776,  2.1975],\n",
      "          [-0.6794, -0.6844, -0.6992,  ...,  2.1186,  2.1776,  2.1975],\n",
      "          [-0.6794, -0.6852, -0.7022,  ...,  2.1102,  2.1715,  2.1922],\n",
      "          ...,\n",
      "          [ 0.5299,  0.5312,  0.5351,  ..., -0.0082, -0.0156, -0.0181],\n",
      "          [ 0.5193,  0.5268,  0.5489,  ..., -0.0188, -0.0262, -0.0287],\n",
      "          [ 0.5193,  0.5268,  0.5489,  ..., -0.0188, -0.0262, -0.0287]],\n",
      "\n",
      "         [[-0.5651, -0.5702, -0.5853,  ...,  2.2953,  2.3556,  2.3761],\n",
      "          [-0.5651, -0.5702, -0.5853,  ...,  2.2953,  2.3556,  2.3761],\n",
      "          [-0.5651, -0.5710, -0.5884,  ...,  2.2868,  2.3494,  2.3706],\n",
      "          ...,\n",
      "          [ 0.6712,  0.6725,  0.6765,  ...,  0.1211,  0.1135,  0.1110],\n",
      "          [ 0.6604,  0.6680,  0.6906,  ...,  0.1102,  0.1027,  0.1001],\n",
      "          [ 0.6604,  0.6680,  0.6906,  ...,  0.1102,  0.1027,  0.1001]],\n",
      "\n",
      "         [[-0.3404, -0.3455, -0.3605,  ...,  2.5073,  2.5674,  2.5877],\n",
      "          [-0.3404, -0.3455, -0.3605,  ...,  2.5073,  2.5674,  2.5877],\n",
      "          [-0.3404, -0.3463, -0.3636,  ...,  2.4989,  2.5612,  2.5823],\n",
      "          ...,\n",
      "          [ 0.8904,  0.8918,  0.8957,  ...,  0.3427,  0.3352,  0.3327],\n",
      "          [ 0.8797,  0.8873,  0.9098,  ...,  0.3320,  0.3245,  0.3219],\n",
      "          [ 0.8797,  0.8873,  0.9098,  ...,  0.3320,  0.3245,  0.3219]]],\n",
      "\n",
      "\n",
      "        [[[-1.9295, -1.9295, -1.9295,  ..., -1.8782, -1.8782, -1.8782],\n",
      "          [-1.9295, -1.9295, -1.9295,  ..., -1.8782, -1.8782, -1.8782],\n",
      "          [-1.9295, -1.9295, -1.9295,  ..., -1.8782, -1.8782, -1.8782],\n",
      "          ...,\n",
      "          [-1.7925, -1.7925, -1.7925,  ..., -1.7583, -1.7583, -1.7583],\n",
      "          [-1.7925, -1.7925, -1.7925,  ..., -1.7583, -1.7583, -1.7583],\n",
      "          [-1.7925, -1.7925, -1.7925,  ..., -1.7583, -1.7583, -1.7583]],\n",
      "\n",
      "         [[-1.8431, -1.8431, -1.8431,  ..., -1.7906, -1.7906, -1.7906],\n",
      "          [-1.8431, -1.8431, -1.8431,  ..., -1.7906, -1.7906, -1.7906],\n",
      "          [-1.8431, -1.8431, -1.8431,  ..., -1.7906, -1.7906, -1.7906],\n",
      "          ...,\n",
      "          [-1.7031, -1.7031, -1.7031,  ..., -1.6681, -1.6681, -1.6681],\n",
      "          [-1.7031, -1.7031, -1.7031,  ..., -1.6681, -1.6681, -1.6681],\n",
      "          [-1.7031, -1.7031, -1.7031,  ..., -1.6681, -1.6681, -1.6681]],\n",
      "\n",
      "         [[-1.6127, -1.6127, -1.6127,  ..., -1.5604, -1.5604, -1.5604],\n",
      "          [-1.6127, -1.6127, -1.6127,  ..., -1.5604, -1.5604, -1.5604],\n",
      "          [-1.6127, -1.6127, -1.6127,  ..., -1.5604, -1.5604, -1.5604],\n",
      "          ...,\n",
      "          [-1.4733, -1.4733, -1.4733,  ..., -1.4384, -1.4384, -1.4384],\n",
      "          [-1.4733, -1.4733, -1.4733,  ..., -1.4384, -1.4384, -1.4384],\n",
      "          [-1.4733, -1.4733, -1.4733,  ..., -1.4384, -1.4384, -1.4384]]]]), tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]]), tensor([0., 0., 0., 0.])]\n"
     ]
    }
   ],
   "source": [
    "im_dir = '/home/ubuntu/workspace/create_train_set/data/dataset_out_mask/images/train'\n",
    "mask_dir = '/home/ubuntu/workspace/create_train_set/data/dataset_out_mask/masks/train'\n",
    "\n",
    "transform_img = transforms.Compose(\n",
    "    [\n",
    "        T.ToTensor(),\n",
    "        T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "        T.Resize((720,720))\n",
    "    ]\n",
    ")\n",
    "\n",
    "transform_mask = transforms.Compose(\n",
    "    [\n",
    "        Threshold(30),\n",
    "        T.ToTensor(),\n",
    "        #T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "        T.Resize((88,88))\n",
    "    ]\n",
    ")\n",
    "\n",
    "ds_source = CustomImageDataset(im_dir, mask_dir, 0, transform_img, transform_mask)\n",
    "\n",
    "\n",
    "dl_source = DataLoader(VirtualDataset, batch_size=4, shuffle=True)\n",
    "\n",
    "print(next(iter(dl_source)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cc77da",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "76b8e96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "n_epochs = 1\n",
    "\n",
    "# Setup optimizer\n",
    "model = DASemantic(720, 2).cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr)\n",
    "\n",
    "# Two Loss Functions (pixelwise class and domain)\n",
    "loss_fn_class = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "loss_fn_domain = nn.NLLLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7252d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# get data\n",
    "\n",
    "# find number of batches to run for\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a836a940",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "for epoch_idx in range(n_epochs):\n",
    "    print(f'Epoch {epoch_idx+1:04d} / {n_epochs:04d}', end='\\n=================\\n')\n",
    "    # source data iter\n",
    "    # target data iter\n",
    "    \n",
    "    for batch_idx in range(max_batches):\n",
    "        p = float(((batch_idx + epoch_idx * max_batches) / (n_epochs * max_batches))/2)\n",
    "        grl_lambda = 2. / (1. + np.exp(-10*p)) - 1\n",
    "        \n",
    "        # Train on source domain\n",
    "        X_s, y_s = next(dl_source_iter)\n",
    "        X_s, y_s = X_s.cuda(), y_s.cuda()\n",
    "        y_s_domain = torch.zeros(batch_size, dtype=torch.long) # generate source domain labels\n",
    "        y_s_domain = y_s_domain.cuda()\n",
    "        \n",
    "        class_pred, domain_pred = model(X_s, grl_lambda)\n",
    "        loss_s_label = loss_fn_class(class_pred, y_s)\n",
    "        loss_s_domain = loss_fn_domain(domain_pred, y_s_domain)\n",
    "        \n",
    "        # Train on target domain\n",
    "        X_t, _ = next(dl_target_iter) # ignore target domain class labels!\n",
    "        X_t = X_t.cuda()\n",
    "        y_t_domain = torch.ones(batch_size, dtype=torch.long) # generate target domain labels\n",
    "        y_t_domain = y_t_domain.cuda()\n",
    "        \n",
    "        _, domain_pred = model(X_t, grl_lambda)\n",
    "        loss_t_domain = loss_fn_domain(domain_pred, y_t_domain)\n",
    "        \n",
    "        # Calculate total loss\n",
    "        loss = loss_t_domain + loss_s_domain + loss_s_label\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f'[{batch_idx+1}/{max_batches}] '\n",
    "                  f'class_loss: {loss_s_label.item():.4f} ' f's_domain_loss: {loss_s_domain.item():.4f} '\n",
    "                  f't_domain_loss: {loss_t_domain.item():.4f} ' f'grl_lambda: {grl_lambda:.3f} '\n",
    "                 )\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c05cd1a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[35.4760, 99.9222],\n",
      "        [43.7156, 72.4414]])  +  tensor([[59.5805, 27.1421],\n",
      "        [19.7191, 31.2870]])\n",
      "tensor([[ 95.0565, 127.0644],\n",
      "        [ 63.4347, 103.7284]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(2,2)*100\n",
    "b = torch.rand(2,2)*100\n",
    "c = torch.add(a,b)\n",
    "print(a, \" + \", b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2fbe35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232aafb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6e313d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1181cdb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2c07bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bac1263",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494905e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
